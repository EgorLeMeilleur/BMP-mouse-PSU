{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "folder_path = \"20230530_segm_black_mouse_mnSLA_red_and_black_back\"\n",
    "\n",
    "seed_value = 52\n",
    "torch.manual_seed(seed_value)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation():\n",
    "    count_image = 350\n",
    "    path_images = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/images\"\n",
    "    path_masks = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\"\n",
    "\n",
    "    image_files = os.listdir(path_images)\n",
    "    selected_files_for_vertical = random.sample(image_files, count_image)\n",
    "\n",
    "    for filename in selected_files_for_vertical:\n",
    "        image_path = os.path.join(path_images, filename)\n",
    "        image = Image.open(image_path)\n",
    "        rotated_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        output_path = os.path.join(path_images, f'revert_vertical_{filename}')\n",
    "        rotated_image.save(output_path)\n",
    "        image.close()\n",
    "\n",
    "        mask_path = os.path.join(path_masks, filename)\n",
    "        mask = Image.open(mask_path)\n",
    "        rotated_mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        output_path = os.path.join(path_masks, f'revert_vertical_{filename}')\n",
    "        rotated_mask.save(output_path)\n",
    "        mask.close()\n",
    "\n",
    "    # selected_files_for_horizontal = []\n",
    "    count_image = 1\n",
    "    selected_files_for_horizontal = random.sample(image_files, count_image)\n",
    "\n",
    "    for filename in selected_files_for_horizontal:\n",
    "        image_path = os.path.join(path_images, filename)\n",
    "        image = Image.open(image_path)\n",
    "        rotated_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        output_path = os.path.join(path_images, f'revert_horizontal_{filename}')\n",
    "        rotated_image.save(output_path)\n",
    "        image.close()\n",
    "\n",
    "        mask_path = os.path.join(path_masks, filename)\n",
    "        mask = Image.open(mask_path)\n",
    "        rotated_mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        output_path = os.path.join(path_masks, f'revert_horizontal_{filename}')\n",
    "        rotated_mask.save(output_path)\n",
    "        mask.close()\n",
    "\n",
    "    return selected_files_for_vertical, selected_files_for_horizontal\n",
    "\n",
    "\n",
    "def delete_generated_images(data_vert, data_hor):\n",
    "    path_images = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/images\"\n",
    "    path_masks = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\"\n",
    "\n",
    "    for filename in data_vert:\n",
    "        output_image_path = os.path.join(path_images, f'revert_vertical_{filename}')\n",
    "        os.remove(output_image_path)\n",
    "\n",
    "        output_mask_path = os.path.join(path_masks, f'revert_vertical_{filename}')\n",
    "        os.remove(output_mask_path)\n",
    "\n",
    "    for filename in data_hor:\n",
    "        output_image_path = os.path.join(path_images, f'revert_horizontal_{filename}')\n",
    "        os.remove(output_image_path)\n",
    "\n",
    "        output_mask_path = os.path.join(path_masks, f'revert_horizontal_{filename}')\n",
    "        os.remove(output_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_files(folder_path, folder):\n",
    "    images_folder = folder_path + \"/\" + folder + \"images\"\n",
    "    masks_folder = folder_path + \"/\" + folder + \"masks\"\n",
    "\n",
    "    images_files = os.listdir(images_folder)\n",
    "    masks_files = os.listdir(masks_folder)\n",
    "\n",
    "    image_paths = [os.path.join(folder + \"images\", file) for file in images_files]\n",
    "    mask_paths = [os.path.join(folder + \"masks\", file) for file in masks_files]\n",
    "\n",
    "    data = {'orig_image': image_paths, 'mask_image': mask_paths}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    csv_file_path = \"train_data.csv\" if folder == \"\" else \"test_data.csv\"\n",
    "\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vertical, data_horizontal = augmentation()\n",
    "make_csv_files(folder_path, \"\")\n",
    "make_csv_files(folder_path, \"test_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(orig_image, orig_masks, mask_image, intersec_mask):\n",
    "    fig, axes = plt.subplots(1, 4)\n",
    "\n",
    "    orig_image = orig_image.transpose(1, 2, 0)\n",
    "    orig_image = (np.array(orig_image) - np.min(orig_image)) / (np.max(orig_image) - np.min(orig_image))\n",
    "    axes[0].imshow(orig_image)\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    axes[1].imshow(orig_masks)\n",
    "    axes[1].set_title('Original Mask')\n",
    "    \n",
    "    axes[2].imshow(mask_image)\n",
    "    axes[2].set_title('Predicted Mask')\n",
    "\n",
    "    axes[3].imshow(intersec_mask)\n",
    "    axes[3].set_title('Difference Mask')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(predictions, targets):\n",
    "    total_sum = 0.0\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        intersection = np.logical_and(prediction, target).sum().item()\n",
    "        union = np.logical_or(prediction, target).sum().item()\n",
    "        \n",
    "        total_sum += intersection / union if union > 0 else 0.0\n",
    "\n",
    "    return total_sum / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_k(predictions, targets, k, klass):\n",
    "    precision = 0\n",
    "    tp = 0\n",
    "    all_det = 0\n",
    "    for i in range(k):\n",
    "        predict = predictions[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        if klass:\n",
    "            tp += np.sum(predict[predict == target])\n",
    "            all_det += np.sum(predict)\n",
    "            precision += tp / all_det\n",
    "        else:\n",
    "            tp += len(predict[predict == target]) - np.count_nonzero(predict[predict == target])\n",
    "            all_det += predict.size - np.count_nonzero(predict)\n",
    "            precision += tp / all_det\n",
    "    return precision / k \n",
    "\n",
    "def compute_ap(predictions, targets, k):\n",
    "    return (ap_k(predictions, targets, k, True) + ap_k(predictions, targets, k, False)) / 2, ap_k(predictions, targets, k, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, folder, data, transform_image, transform_mask, size):\n",
    "      self.folder = folder\n",
    "      self.data = data.copy()\n",
    "      self.orig_image_paths = [os.path.join(folder, filename) for filename in data['orig_image'].copy()]\n",
    "      self.mask_image_paths = [os.path.join(folder, filename) for filename in data['mask_image'].copy()]\n",
    "      self.transform_image = transform_image\n",
    "      self.transform_mask = transform_mask\n",
    "      self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.orig_image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        orig_image_path = self.orig_image_paths[idx]\n",
    "        mask_image_path = self.mask_image_paths[idx]\n",
    "        orig_image = Image.open(orig_image_path).convert('RGB')\n",
    "        mask_image = Image.open(mask_image_path).convert('L')\n",
    "        \n",
    "        orig_image = self.transform_image(orig_image)\n",
    "        orig_image = orig_image.to(orig_image)\n",
    "        \n",
    "        mask_image = self.transform_mask(mask_image)\n",
    "        mask_image = mask_image.to(mask_image)\n",
    "        \n",
    "        return {'image': orig_image.float().to(device), 'mask_input': mask_image.float().to(device), 'original_size': self.size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(train_df, test_size=0.1 , random_state=42)\n",
    "\n",
    "size = (32, 54)\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "batch_size = 8\n",
    "\n",
    "transform_image = transforms.Compose([transforms.Resize(size), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "transform_mask = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ImagesDataset(folder_path, train, transform_image, transform_mask, size)\n",
    "val_dataset = ImagesDataset(folder_path, val, transform_image, transform_mask, size)\n",
    "test_dataset = ImagesDataset(folder_path, test, transform_image, transform_mask, size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(num_epochs, train_load, val_load, model, optimizer, criterion, model_name, scheduler=None):\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  iou_test = []\n",
    "  max_iou = 0.0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_load):\n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(batch, multimask_output=False)\n",
    "        outputs = model(pixel_values=batch[\"image\"],\n",
    "                        input_boxes=batch[\"mask_input\"],\n",
    "                        multimask_output=False)\n",
    "        # prediction_masks = torch.stack([x[\"masks\"] for x in outputs], dim=0)\n",
    "        orig_masks = torch.stack([x[\"mask_input\"] for x in batch], dim=0)\n",
    "        prediction_masks = outputs.pred_masks.squeeze(1)\n",
    "        loss = criterion(orig_masks, prediction_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_load))\n",
    "    \n",
    "    if scheduler is not None:\n",
    "      scheduler.step(loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_load):\n",
    "          outputs = model(batch, multimask_output=False)\n",
    "          prediction_masks = torch.stack([x[\"masks\"] for x in outputs], dim=0)\n",
    "          orig_masks = torch.stack([x[\"mask_input\"] for x in batch], dim=0)\n",
    "          loss = criterion(orig_masks, prediction_masks)\n",
    "          val_loss += (loss.item())\n",
    "    val_losses.append(val_loss/len(val_load))\n",
    "\n",
    "    predictions, _, orig_masks, _ = prediction(model, test_loader)\n",
    "    iou = calculate_iou(predictions, orig_masks)\n",
    "    iou_test.append(iou)\n",
    "\n",
    "    if iou > max_iou:\n",
    "       max_iou = iou\n",
    "       torch.save(model.state_dict(), f\"models/{model_name}_IOU-{iou}.pth\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, IOU: {iou:.4f}\")\n",
    "  return model, train_losses, val_losses, iou_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    orig_images = []\n",
    "    orig_masks = []\n",
    "    intersection_masks = []\n",
    "    size = (544, 928)\n",
    "    transform = transforms.Compose([transforms.Resize(size)])\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            outputs = model(batch, multimask_output=False)\n",
    "            prediction_masks = torch.stack([x[\"masks\"] for x in outputs], dim=0)\n",
    "            orig_images = torch.stack([x[\"images\"] for x in batch], dim=0)\n",
    "            orig_masks = torch.stack([x[\"mask_input\"] for x in batch], dim=0)\n",
    "            #new_outputs = torch.zeros(prediction_masks.shape[0], prediction_masks.shape[1], prediction_masks.shape[2], prediction_masks.shape[3])\n",
    "            prediction_masks[prediction_masks < 0] = 0\n",
    "            prediction_masks[prediction_masks > 0] = 1\n",
    "\n",
    "            # for i in range(outputs.shape[0]):\n",
    "            #     for j in range(outputs.shape[2]):\n",
    "            #         for k in range(outputs.shape[3]):\n",
    "            #             if outputs[i, 0, j, k] == 1:\n",
    "            #                 for m in range(-1, 2):\n",
    "            #                     for n in range(-1, 2):\n",
    "            #                         if 0 <= j + m < outputs.shape[2] - 4 and 0 <= k + n < outputs.shape[3] - 4:\n",
    "            #                             new_outputs[i, 0, j + 1 + m, k + 1 + n] = 1\n",
    "\n",
    "            # outputs = new_outputs\n",
    "\n",
    "            prediction_masks = transform(prediction_masks)\n",
    "            orig_images = transform(orig_images)\n",
    "            orig_masks = transform(orig_masks)\n",
    "\n",
    "            predictions.append(prediction_masks.cpu().numpy())\n",
    "            orig_images.append(orig_images.cpu().numpy())\n",
    "            orig_masks.append(orig_masks.cpu().numpy())\n",
    "\n",
    "            intersection = np.abs(prediction_masks.cpu().numpy() - orig_masks.cpu().numpy())\n",
    "            intersection_masks.append(intersection)\n",
    "    predictions = np.concatenate(predictions, axis=0).squeeze()\n",
    "    orig_images = np.concatenate(orig_images, axis=0).squeeze()\n",
    "    orig_masks = np.concatenate(orig_masks, axis=0).squeeze()\n",
    "    intersection_masks = np.concatenate(intersection_masks, axis=0).squeeze()\n",
    "    return predictions, orig_images, orig_masks, intersection_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader, images_to_draw):\n",
    "    predictions, orig_images, orig_masks, intersection_masks = prediction(model, loader)\n",
    "\n",
    "    iou = calculate_iou(predictions, orig_masks)\n",
    "    apk, ap = compute_ap(predictions, orig_masks, len(predictions))\n",
    "\n",
    "    print(f\"IOU: {iou}\")\n",
    "    print(f\"AP for Two Classes: {apk}\")\n",
    "    print(f\"AP for Mouse Class: {ap}\")\n",
    "\n",
    "    for i in range(images_to_draw):\n",
    "        draw(orig_images[i], orig_masks[i], predictions[i], intersection_masks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint='models/sam_vit_h_4b8939.pth')\n",
    "sam.cuda()\n",
    "model_name = 'sam_vit_h_4b8939'\n",
    "for param in sam.parameters():\n",
    "    param.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "\n",
    "sam = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model_name = \"sam-vit-base\"\n",
    "\n",
    "for name, param in sam.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai \n",
    "\n",
    "optimizer = Adam(sam.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "criterion = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from torch.nn.functional import threshold, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# optimizer = optim.Adamax(sam.parameters(), lr=learning_rate)\n",
    "# criterion = smp.losses.DiceLoss(mode='binary')\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/139 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('The input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.', ' got torch.Size([8, 1, 32, 54]).')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      3\u001b[0m sam\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m sam, train_losses, val_losses, iou_test \u001b[38;5;241m=\u001b[39m \u001b[43mlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m, in \u001b[0;36mlearning\u001b[1;34m(num_epochs, train_load, val_load, model, optimizer, criterion, model_name, scheduler)\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# outputs = model(batch, multimask_output=False)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# prediction_masks = torch.stack([x[\"masks\"] for x in outputs], dim=0)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m orig_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_input\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:1334\u001b[0m, in \u001b[0;36mSamModel.forward\u001b[1;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1330\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input_points must be a 4D tensor. Of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input_points\u001b[38;5;241m.\u001b[39mshape),\n\u001b[0;32m   1332\u001b[0m     )\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_boxes\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input_boxes\u001b[38;5;241m.\u001b[39mshape),\n\u001b[0;32m   1337\u001b[0m     )\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m input_boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1339\u001b[0m     point_batch_size \u001b[38;5;241m=\u001b[39m input_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: ('The input_points must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.', ' got torch.Size([8, 1, 32, 54]).')"
     ]
    }
   ],
   "source": [
    "sam.to(device)\n",
    "num_epochs = 5\n",
    "sam.train()\n",
    "sam, train_losses, val_losses, iou_test = learning(num_epochs, train_loader, val_loader, sam, optimizer, criterion, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(train_losses, label='Training Losses', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(val_losses, label='Validation Losses', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(iou_test, label='IOU test', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('IOU Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, train_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, val_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, test_loader, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
