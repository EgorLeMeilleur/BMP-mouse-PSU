{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "folder_path = \"20230530_segm_black_mouse_mnSLA_red_and_black_back\"\n",
    "\n",
    "seed_value = 52\n",
    "torch.manual_seed(seed_value)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation():\n",
    "    count_image = 350\n",
    "    path_images = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/images\"\n",
    "    path_masks = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\"\n",
    "\n",
    "    image_files = os.listdir(path_images)\n",
    "    selected_files_for_vertical = random.sample(image_files, count_image)\n",
    "\n",
    "    for filename in selected_files_for_vertical:\n",
    "        image_path = os.path.join(path_images, filename)\n",
    "        image = Image.open(image_path)\n",
    "        rotated_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        output_path = os.path.join(path_images, f'revert_vertical_{filename}')\n",
    "        rotated_image.save(output_path)\n",
    "        image.close()\n",
    "\n",
    "        mask_path = os.path.join(path_masks, filename)\n",
    "        mask = Image.open(mask_path)\n",
    "        rotated_mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        output_path = os.path.join(path_masks, f'revert_vertical_{filename}')\n",
    "        rotated_mask.save(output_path)\n",
    "        mask.close()\n",
    "\n",
    "    # selected_files_for_horizontal = []\n",
    "    count_image = 1\n",
    "    selected_files_for_horizontal = random.sample(image_files, count_image)\n",
    "\n",
    "    for filename in selected_files_for_horizontal:\n",
    "        image_path = os.path.join(path_images, filename)\n",
    "        image = Image.open(image_path)\n",
    "        rotated_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        output_path = os.path.join(path_images, f'revert_horizontal_{filename}')\n",
    "        rotated_image.save(output_path)\n",
    "        image.close()\n",
    "\n",
    "        mask_path = os.path.join(path_masks, filename)\n",
    "        mask = Image.open(mask_path)\n",
    "        rotated_mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        output_path = os.path.join(path_masks, f'revert_horizontal_{filename}')\n",
    "        rotated_mask.save(output_path)\n",
    "        mask.close()\n",
    "\n",
    "    return selected_files_for_vertical, selected_files_for_horizontal\n",
    "\n",
    "\n",
    "def delete_generated_images(data_vert, data_hor):\n",
    "    path_images = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/images\"\n",
    "    path_masks = \"20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\"\n",
    "\n",
    "    for filename in data_vert:\n",
    "        output_image_path = os.path.join(path_images, f'revert_vertical_{filename}')\n",
    "        os.remove(output_image_path)\n",
    "\n",
    "        output_mask_path = os.path.join(path_masks, f'revert_vertical_{filename}')\n",
    "        os.remove(output_mask_path)\n",
    "\n",
    "    for filename in data_hor:\n",
    "        output_image_path = os.path.join(path_images, f'revert_horizontal_{filename}')\n",
    "        os.remove(output_image_path)\n",
    "\n",
    "        output_mask_path = os.path.join(path_masks, f'revert_horizontal_{filename}')\n",
    "        os.remove(output_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_files(folder_path, folder):\n",
    "    images_folder = folder_path + \"/\" + folder + \"images\"\n",
    "    masks_folder = folder_path + \"/\" + folder + \"masks\"\n",
    "\n",
    "    images_files = os.listdir(images_folder)\n",
    "    masks_files = os.listdir(masks_folder)\n",
    "\n",
    "    image_paths = [os.path.join(folder + \"images\", file) for file in images_files]\n",
    "    mask_paths = [os.path.join(folder + \"masks\", file) for file in masks_files]\n",
    "\n",
    "    data = {'orig_image': image_paths, 'mask_image': mask_paths}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    csv_file_path = \"train_data.csv\" if folder == \"\" else \"test_data.csv\"\n",
    "\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(orig_image, orig_masks, mask_image, intersec_mask):\n",
    "    fig, axes = plt.subplots(1, 4)\n",
    "\n",
    "    orig_image = orig_image.transpose(1, 2, 0)\n",
    "    orig_image = (np.array(orig_image) - np.min(orig_image)) / (np.max(orig_image) - np.min(orig_image))\n",
    "    axes[0].imshow(orig_image)\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    axes[1].imshow(orig_masks)\n",
    "    axes[1].set_title('Original Mask')\n",
    "    \n",
    "    axes[2].imshow(mask_image)\n",
    "    axes[2].set_title('Predicted Mask')\n",
    "\n",
    "    axes[3].imshow(intersec_mask)\n",
    "    axes[3].set_title('Difference Mask')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(predictions, targets):\n",
    "    total_sum = 0.0\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        intersection = np.logical_and(prediction, target).sum().item()\n",
    "        union = np.logical_or(prediction, target).sum().item()\n",
    "        \n",
    "        total_sum += intersection / union if union > 0 else 0.0\n",
    "\n",
    "    return total_sum / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_k(predictions, targets, k, klass):\n",
    "    precision = 0\n",
    "    tp = 0\n",
    "    all_det = 0\n",
    "    for i in range(k):\n",
    "        predict = predictions[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        if klass:\n",
    "            tp += np.sum(predict[predict == target])\n",
    "            all_det += np.sum(predict)\n",
    "            precision += tp / all_det\n",
    "        else:\n",
    "            tp += len(predict[predict == target]) - np.count_nonzero(predict[predict == target])\n",
    "            all_det += predict.size - np.count_nonzero(predict)\n",
    "            precision += tp / all_det\n",
    "    return precision / k \n",
    "\n",
    "def compute_ap(predictions, targets, k):\n",
    "    return (ap_k(predictions, targets, k, True) + ap_k(predictions, targets, k, False)) / 2, ap_k(predictions, targets, k, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "  bbox_array = []\n",
    "  for ground_truth_mask in ground_truth_map:\n",
    "    y_indices, x_indices = np.where(ground_truth_mask > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    H, W = ground_truth_mask.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "    bbox_array.append(bbox)\n",
    "  return bbox_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_vertical, data_horizontal = augmentation()\n",
    "#make_csv_files(folder_path, \"\")\n",
    "#make_csv_files(folder_path, \"test_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = os.listdir('20230530_segm_black_mouse_mnSLA_red_and_black_back/images')\n",
    "train_masks = os.listdir('20230530_segm_black_mouse_mnSLA_red_and_black_back/masks')\n",
    "test_images = os.listdir('20230530_segm_black_mouse_mnSLA_red_and_black_back/test_images')\n",
    "test_masks = os.listdir('20230530_segm_black_mouse_mnSLA_red_and_black_back/test_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "encoder error -2 when writing image file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m tiff_files \u001b[38;5;241m=\u001b[39m get_tiff_files(folder_path)\n\u001b[0;32m     42\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmerge_tiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiff_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m, in \u001b[0;36mmerge_tiff\u001b[1;34m(input_files, output_file)\u001b[0m\n\u001b[0;32m     21\u001b[0m     result_image\u001b[38;5;241m.\u001b[39mpaste(img, (\u001b[38;5;241m0\u001b[39m, i \u001b[38;5;241m*\u001b[39m height))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Сохраняем объединенное изображение\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mresult_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTIFF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtiff_lzw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# result_image.save(output_file)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Закрываем все изображения\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:2459\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2456\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2459\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\TiffImagePlugin.py:1879\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename)\u001b[0m\n\u001b[0;32m   1877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1878\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when writing image file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m   1881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m blocklist:\n",
      "\u001b[1;31mOSError\u001b[0m: encoder error -2 when writing image file"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def merge_tiff(input_files, output_file):\n",
    "    images = [Image.open(file) for file in input_files]\n",
    "    \n",
    "    # Получаем размеры и режим первого изображения\n",
    "    width, height = images[0].size\n",
    "    mode = images[0].mode\n",
    "    \n",
    "    # Проверяем, что все изображения имеют одинаковые размеры и режим\n",
    "    for img in images[1:]:\n",
    "        if img.size != (width, height):\n",
    "            img = img.resize((width, height), Image.ANTIALIAS)\n",
    "            print(width, height)\n",
    "    \n",
    "    # Создаем новое изображение, к которому будем добавлять кадры\n",
    "    result_image = Image.new(mode, (width, height * len(images)))\n",
    "\n",
    "    # Сливаем каждое изображение в новое изображение\n",
    "    for i, img in enumerate(images):\n",
    "        result_image.paste(img, (0, i * height))\n",
    "\n",
    "    # Сохраняем объединенное изображение\n",
    "    result_image.save(output_file, format='TIFF', compression='tiff_lzw')\n",
    "    # result_image.save(output_file)\n",
    "\n",
    "    # Закрываем все изображения\n",
    "    for img in images:\n",
    "        img.close()\n",
    "\n",
    "def get_tiff_files(folder):\n",
    "    tiff_files = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".tif\") or file.endswith(\".tiff\"):\n",
    "            tiff_files.append(os.path.join(folder, file))\n",
    "    return tiff_files\n",
    "\n",
    "folder_path = \"tiff_data/images\"\n",
    "\n",
    "tiff_files = get_tiff_files(folder_path)\n",
    "\n",
    "output_file = \"images.tif\"\n",
    "\n",
    "merge_tiff(tiff_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m train_images \u001b[38;5;241m=\u001b[39m train_images[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      9\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_labels[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m train_dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/images\u001b[39m\u001b[38;5;124m'\u001b[39m, img))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m train_images],\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\u001b[39m\u001b[38;5;124m'\u001b[39m, mask))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m train_labels],\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m val_dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/images\u001b[39m\u001b[38;5;124m'\u001b[39m, img))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m val_images],\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\u001b[39m\u001b[38;5;124m'\u001b[39m, mask))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m val_labels],\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     21\u001b[0m test_dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/test_images\u001b[39m\u001b[38;5;124m'\u001b[39m, img))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m test_images],\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/test_masks\u001b[39m\u001b[38;5;124m'\u001b[39m, mask))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m test_masks],\n\u001b[0;32m     24\u001b[0m }\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m train_images \u001b[38;5;241m=\u001b[39m train_images[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      9\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_labels[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m train_dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m20230530_segm_black_mouse_mnSLA_red_and_black_back/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m train_images],\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\u001b[39m\u001b[38;5;124m'\u001b[39m, mask))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m train_labels],\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m val_dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/images\u001b[39m\u001b[38;5;124m'\u001b[39m, img))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m val_images],\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/masks\u001b[39m\u001b[38;5;124m'\u001b[39m, mask))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m val_labels],\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     21\u001b[0m test_dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/test_images\u001b[39m\u001b[38;5;124m'\u001b[39m, img))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m test_images],\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: [Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230530_segm_black_mouse_mnSLA_red_and_black_back/test_masks\u001b[39m\u001b[38;5;124m'\u001b[39m, mask))\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m test_masks],\n\u001b[0;32m     24\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:2185\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   2183\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[1;32m-> 2185\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2187\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_masks, test_size=0.1, random_state=42)\n",
    "val_images = val_images[:-1]\n",
    "val_labels = val_labels[:-1]\n",
    "train_images = train_images[:-1]\n",
    "train_labels = train_labels[:-1]\n",
    "\n",
    "train_dataset_dict = {\n",
    "    \"image\": [Image.open(os.path.join('20230530_segm_black_mouse_mnSLA_red_and_black_back/images', img)).resize((256, 256)) for img in train_images],\n",
    "    \"label\": [Image.open(os.path.join('20230530_segm_black_mouse_mnSLA_red_and_black_back/masks', mask)).resize((256, 256)) for mask in train_labels],\n",
    "}\n",
    "\n",
    "val_dataset_dict = {\n",
    "    \"image\": [Image.open(os.path.join('20230530_segm_black_mouse_mnSLA_red_and_black_back/images', img)).resize((256, 256)) for img in val_images],\n",
    "    \"label\": [Image.open(os.path.join('20230530_segm_black_mouse_mnSLA_red_and_black_back/masks', mask)).resize((256, 256)) for mask in val_labels],\n",
    "}\n",
    "\n",
    "test_dataset_dict = {\n",
    "    \"image\": [Image.open(os.path.join('20230530_segm_black_mouse_mnSLA_red_and_black_back/test_images', img)).resize((256, 256)) for img in test_images],\n",
    "    \"label\": [Image.open(os.path.join('20230530_segm_black_mouse_mnSLA_red_and_black_back/test_masks', mask)).resize((256, 256)) for mask in test_masks],\n",
    "}\n",
    "\n",
    "train_set = Dataset.from_dict(train_dataset_dict)\n",
    "val_set = Dataset.from_dict(val_dataset_dict)\n",
    "test_set = Dataset.from_dict(test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 1104\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 122\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 49\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(Dataset):\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "    ground_truth_mask[ground_truth_mask > 0] = 1\n",
    "    \n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(num_epochs, train_load, val_load, model, optimizer, criterion, model_name, scheduler=None):\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  iou_test = []\n",
    "  max_iou = 0.0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_load):\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].unsqueeze(1).to(device),\n",
    "                      multimask_output=False)\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        predicted_masks = torch.sigmoid(predicted_masks)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(ground_truth_masks, predicted_masks.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_load))\n",
    "    \n",
    "    if scheduler is not None:\n",
    "      scheduler.step(loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_load):\n",
    "          outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].unsqueeze(1).to(device),\n",
    "                      multimask_output=False)\n",
    "          predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "          predicted_masks = torch.sigmoid(predicted_masks)\n",
    "          ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "          loss = criterion(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "          val_loss += (loss.item())\n",
    "    val_losses.append(val_loss/len(val_load))\n",
    "\n",
    "    predictions, _, orig_masks, _ = prediction(model, test_loader)\n",
    "    iou = calculate_iou(predictions, orig_masks)\n",
    "    iou_test.append(iou)\n",
    "\n",
    "    if iou > max_iou:\n",
    "       max_iou = iou\n",
    "       torch.save(model.state_dict(), f\"models/{model_name}_IOU-{iou}.pth\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, IOU: {iou:.4f}\")\n",
    "  return model, train_losses, val_losses, iou_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    orig_images = []\n",
    "    orig_masks = []\n",
    "    intersection_masks = []\n",
    "    size = (544, 928)\n",
    "    threshold = 0\n",
    "    transform = transforms.Compose([transforms.Resize(size)])\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].unsqueeze(1).to(device),\n",
    "                      multimask_output=False)\n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device).unsqueeze(1)\n",
    "            #predicted_masks = torch.sigmoid(predicted_masks)\n",
    "            #new_outputs = torch.zeros(prediction_masks.shape[0], prediction_masks.shape[1], prediction_masks.shape[2], prediction_masks.shape[3])\n",
    "            predicted_masks[predicted_masks < threshold] = 0\n",
    "            predicted_masks[predicted_masks > threshold] = 1\n",
    "\n",
    "            # for i in range(outputs.shape[0]):\n",
    "            #     for j in range(outputs.shape[2]):\n",
    "            #         for k in range(outputs.shape[3]):\n",
    "            #             if outputs[i, 0, j, k] == 1:\n",
    "            #                 for m in range(-1, 2):\n",
    "            #                     for n in range(-1, 2):\n",
    "            #                         if 0 <= j + m < outputs.shape[2] - 4 and 0 <= k + n < outputs.shape[3] - 4:\n",
    "            #                             new_outputs[i, 0, j + 1 + m, k + 1 + n] = 1\n",
    "\n",
    "            # outputs = new_outputs\n",
    "\n",
    "            predicted_masks = transform(predicted_masks)\n",
    "            orig_image = transform(batch[\"pixel_values\"].float())\n",
    "            orig_mask = transform(ground_truth_masks)\n",
    "\n",
    "            predictions.append(predicted_masks.cpu().numpy())\n",
    "            orig_images.append(orig_image.cpu().numpy())\n",
    "            orig_masks.append(orig_mask.cpu().numpy())\n",
    "\n",
    "            intersection = np.abs(predicted_masks.cpu().numpy() - orig_mask.cpu().numpy())\n",
    "            intersection_masks.append(intersection)\n",
    "    predictions = np.concatenate(predictions, axis=0).squeeze()\n",
    "    orig_images = np.concatenate(orig_images, axis=0).squeeze()\n",
    "    orig_masks = np.concatenate(orig_masks, axis=0).squeeze()\n",
    "    intersection_masks = np.concatenate(intersection_masks, axis=0).squeeze()\n",
    "    return predictions, orig_images, orig_masks, intersection_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader, images_to_draw):\n",
    "    predictions, orig_images, orig_masks, intersection_masks = prediction(model, loader)\n",
    "\n",
    "    iou = calculate_iou(predictions, orig_masks)\n",
    "    apk, ap = compute_ap(predictions, orig_masks, len(predictions))\n",
    "\n",
    "    print(f\"IOU: {iou}\")\n",
    "    print(f\"AP for Two Classes: {apk}\")\n",
    "    print(f\"AP for Mouse Class: {ap}\")\n",
    "\n",
    "    for i in range(images_to_draw):\n",
    "        draw(orig_images[i], orig_masks[i], predictions[i], intersection_masks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = SAMDataset(dataset=train_set, processor=processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataset = SAMDataset(dataset=val_set, processor=processor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_dataset = SAMDataset(dataset=test_set, processor=processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "model_name = \"sam-vit-base\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai \n",
    "\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=lr, weight_decay=0)\n",
    "criterion = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 552/552 [03:46<00:00,  2.44it/s]\n",
      "100%|██████████| 61/61 [00:24<00:00,  2.50it/s]\n",
      "100%|██████████| 24/24 [00:09<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train loss: 2300.9990, Val loss: 97.8408, IOU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 523/552 [03:35<00:11,  2.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, train_losses, val_losses, iou_test \u001b[38;5;241m=\u001b[39m \u001b[43mlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36mlearning\u001b[1;34m(num_epochs, train_load, val_load, model, optimizer, criterion, model_name, scheduler)\u001b[0m\n\u001b[0;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_load):\n\u001b[1;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_boxes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     predicted_masks \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpred_masks\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     predicted_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(predicted_masks)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:1357\u001b[0m, in \u001b[0;36mSamModel.forward\u001b[1;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1354\u001b[0m vision_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1357\u001b[0m     vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1363\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:1046\u001b[0m, in \u001b[0;36mSamVisionEncoder.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1041\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1042\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1043\u001b[0m         hidden_states,\n\u001b[0;32m   1044\u001b[0m     )\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1046\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1048\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:939\u001b[0m, in \u001b[0;36mSamVisionLayer.forward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    936\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    937\u001b[0m     hidden_states, padding_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_partition(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[1;32m--> 939\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:842\u001b[0m, in \u001b[0;36mSamVisionAttention.forward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    839\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m (query \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[1;32m--> 842\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attn_weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    848\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:816\u001b[0m, in \u001b[0;36mSamVisionAttention.add_decomposed_rel_pos\u001b[1;34m(self, attn, query, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[0;32m    814\u001b[0m query_height, query_width \u001b[38;5;241m=\u001b[39m q_size\n\u001b[0;32m    815\u001b[0m key_height, key_width \u001b[38;5;241m=\u001b[39m k_size\n\u001b[1;32m--> 816\u001b[0m relative_position_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    817\u001b[0m relative_position_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rel_pos(query_width, key_width, rel_pos_w)\n\u001b[0;32m    819\u001b[0m batch_size, _, dim \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\sam\\modeling_sam.py:781\u001b[0m, in \u001b[0;36mSamVisionAttention.get_rel_pos\u001b[1;34m(self, q_size, k_size, rel_pos)\u001b[0m\n\u001b[0;32m    778\u001b[0m k_coords \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(k_size)[\u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size \u001b[38;5;241m/\u001b[39m k_size, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    779\u001b[0m relative_coords \u001b[38;5;241m=\u001b[39m (q_coords \u001b[38;5;241m-\u001b[39m k_coords) \u001b[38;5;241m+\u001b[39m (k_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size \u001b[38;5;241m/\u001b[39m k_size, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m--> 781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrel_pos_resized\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelative_coords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, train_losses, val_losses, iou_test = learning(num_epochs, train_loader, val_loader, model, optimizer, criterion, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(train_losses, label='Training Losses', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(val_losses, label='Validation Losses', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(iou_test, label='IOU test', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('IOU Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, train_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, val_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, test_loader, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
